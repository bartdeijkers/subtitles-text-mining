{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    Title: subtitle analysis with python\n",
    "    Author: Bart Deijkers (bartdeijkers@gmail.com)\n",
    "    Date: 2022-10-19\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    BASE_DIR = \"/content\"\n",
    "    print(\"You are working on Google Colab.\")\n",
    "    print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
    "else:\n",
    "    BASE_DIR = \"..\"\n",
    "    print(\"You are working on a local system.\")\n",
    "    print(f'Files will be searched relative to \"{BASE_DIR}\".')\n",
    "\n",
    "\n",
    "if spacy.prefer_gpu():\n",
    " print(\"Working on GPU.\")\n",
    "else:\n",
    " print(\"No GPU found, working on CPU.\")\n",
    "\n",
    "stopword_list = stopwords.words('dutch')\n",
    "\n",
    "# Load dutch tokenizer, tagger, parser and NER\n",
    "# download and install the model with: \n",
    "# $ python -m spacy download nl_core_news_lg\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "# increase the maximum length of the corpus if available/needed (default is 1M)\n",
    "# based on step 3 below\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# provide text from files in the data directory\n",
    "text = ''\n",
    "\n",
    "# skip lines that contain these strings\n",
    "skip = [\"888\",\"LIVEPROGRAMMA,\\nONDERTITELING KAN ACHTERLOPEN\",\"NPO ONDERTITELING TT888, 2022\\ninformatie: service.npo.nl\",\"DIT PROGRAMMA WERD LIVE ONDERTITELD\"]\n",
    "\n",
    "# read all json files in folder (change to your own folder if needed)\n",
    "dataset = \"./data/json/\"\n",
    "\n",
    "# loop files\n",
    "for filename in os.listdir(dataset):\n",
    "    # open file\n",
    "    with open(dataset + filename, \"r\") as raw_data:\n",
    "        # read file as json object\n",
    "        data = json.load(raw_data)\n",
    "\n",
    "        # loop json objects and append text to text variable (skipping last item)\n",
    "        for item in data:\n",
    "            if not any(re.search(x, item['text']) for x in skip):\n",
    "                text += item['text']+' '\n",
    "            \n",
    "# remove sentence continuation markers (Dutch TT888 convention)        \n",
    "text = text.replace('...',' ')\n",
    "\n",
    "# remove newlines\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "# regex 2 or more spaces to 1 space\n",
    "text = re.sub(' +', ' ', text)\n",
    "\n",
    "print(\"done loading json data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 : Check dataset boundaries (should be under 2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text length:\", len(text))\n",
    "\n",
    "if len(text) > 2000000:\n",
    "    print(\"this text is too long for spacy, please split it up in smaller chunks, or increase the maximum length of the corpus\")\n",
    "else:\n",
    "    print(\"No boundaries reached, processing text\")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# add stopwords common in dutch subtitles\n",
    "nlp.Defaults.stop_words.add('bedankt')\n",
    "nlp.Defaults.stop_words.add('888')\n",
    "nlp.Defaults.stop_words.add('hee')\n",
    "nlp.Defaults.stop_words.add('gelach')\n",
    "nlp.Defaults.stop_words.add('applaus')\n",
    "nlp.Defaults.stop_words.add('applaus en gejuich')\n",
    "nlp.Defaults.stop_words.add('gejuich')\n",
    "nlp.Defaults.stop_words.add('mens')\n",
    "nlp.Defaults.stop_words.add('een beetje')\n",
    "nlp.Defaults.stop_words.add('dank')\n",
    "nlp.Defaults.stop_words.add('dank u')\n",
    "nlp.Defaults.stop_words.add('dank u wel')\n",
    "nlp.Defaults.stop_words.add('dank u wel allemaal')\n",
    "\n",
    "events = [t for t in doc.ents if t.label_ == 'EVENT']\n",
    "persons = [t for t in doc.ents if t.label_ == 'PERSON']\n",
    "locations = [t for t in doc.ents if t.label_ == 'LOC']\n",
    "organizations = [t for t in doc.ents if t.label_ == 'ORG']\n",
    "nouns =  [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Get top 25 mentioned events, persons, organizations, locations and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all words in doc and add to word_freq dictionary\n",
    "# if word is already in dictionary, add 1 to value\n",
    "# filter out stopwords\n",
    "def get_word_freq(doc):\n",
    "    word_freq = {}\n",
    "    for word in doc:\n",
    "        if word.lemma_ not in nlp.Defaults.stop_words:\n",
    "            if word.lemma_ not in stopword_list:\n",
    "                if word.lemma_ in word_freq.keys():\n",
    "                    word_freq[word.lemma_] += 1\n",
    "                else:\n",
    "                    word_freq[word.lemma_] = 1\n",
    "    result = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    # return first column of result\n",
    "    return result\n",
    "    \n",
    "# sort word frequency\n",
    "sorted_event_freq = get_word_freq(events)\n",
    "sorted_person_freq = get_word_freq(persons)\n",
    "sorted_organization_freq = get_word_freq(organizations)\n",
    "sorted_location_freq = get_word_freq(locations)\n",
    "sorted_noun_freq = get_word_freq(nouns)\n",
    "\n",
    "data = [sorted_event_freq[:25]\n",
    "       ,sorted_person_freq[:25]\n",
    "       ,sorted_organization_freq[:25]\n",
    "       ,sorted_location_freq[:25]\n",
    "       ,sorted_noun_freq[:25]]\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(list(zip(*data)), columns =['Events', 'Persons', 'Organizations', 'Locations', 'Nouns'])\n",
    "print(df)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud from events (top 50)\n",
    "wordcloud = WordCloud(width = 1024, height = 1024,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(dict(sorted_event_freq[:50]))\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (10, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud from persons (top 50)\n",
    "wordcloud = WordCloud(width = 1024, height = 1024,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(dict(sorted_person_freq[:50]))\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (10, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud from organizations (top 50)\n",
    "wordcloud = WordCloud(width = 1024, height = 1024,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(dict(sorted_organization_freq[:50]))\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (10, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud from locations (top 50)\n",
    "wordcloud = WordCloud(width = 1024, height = 1024,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(dict(sorted_location_freq[:50]))\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (10, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud from all noun chunks (top 50)\n",
    "wordcloud = WordCloud(width = 1024, height = 1024,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(dict(get_word_freq(nouns)[:50]))\n",
    "                \n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (10, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n",
    "print('done')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54154c211202f9adaa8e552dfe21778857ce24dd1704463851ce965a6d5abeae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
